#!/usr/bin/env python3
"""
ë‰´ìŠ¤ ì•„ì´ë””ì–´ ì—ì´ì „íŠ¸
- ë§¤ ì‹œê°„ ì •ê° GitHub Actionsì— ì˜í•´ ìë™ ì‹¤í–‰
- Supabase agent_settings í…Œì´ë¸”ì—ì„œ ì„¤ì • ë¡œë“œ
- í™œì„±í™”ëœ í•œêµ­ ë‰´ìŠ¤ RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ 3ê°œ ìˆ˜ì§‘
- Claude AIë¡œ 3ê°œ ë‰´ìŠ¤ë¥¼ ê²°í•©í•˜ì—¬ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ë„ì¶œ
- Supabaseì— ì €ì¥ + í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë°œì†¡
"""

import os
import re
import sys
import json
import socket
import hashlib
import feedparser
import anthropic
import urllib.request
import urllib.parse
from datetime import datetime, timezone, timedelta


ALL_SOURCES = {
    # Google News (í•´ì™¸ ì„œë²„ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥ â€” ìµœìš°ì„  ì‹œë„)
    "êµ¬ê¸€ë‰´ìŠ¤": "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko",
    "êµ¬ê¸€ë‰´ìŠ¤_ê²½ì œ": "https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtdHZHZ0pMVWlnQVAB?hl=ko&gl=KR&ceid=KR:ko",
    "êµ¬ê¸€ë‰´ìŠ¤_IT": "https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGd3TVRBU0FtdHZHZ0pMVWlnQVAB?hl=ko&gl=KR&ceid=KR:ko",
    # ë°©ì†¡ì‚¬
    "KBS":    "https://news.kbs.co.kr/rss/rss.do",
    "MBC":    "https://imnews.imbc.com/rss/news/news_00.xml",
    "SBS":    "https://news.sbs.co.kr/news/headlineRssFeed.do?plink=RSSREADER",
    "JTBC":   "https://news.jtbc.co.kr/RSS/NewsFlash.xml",
    # í†µì‹ ì‚¬
    "ì—°í•©ë‰´ìŠ¤":  "https://www.yna.co.kr/RSS/news.xml",
    "ë‰´ìŠ¤1":   "https://www.news1.kr/rss/allNews.xml",
    "ë‰´ì‹œìŠ¤":  "https://www.newsis.com/RSS/",
    # ì¢…í•©ì¼ê°„ì§€
    "ì¡°ì„ ì¼ë³´": "https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml",
    "ì¤‘ì•™ì¼ë³´": "https://rss.joinsmsn.com/news/sectlist/uAll.xml",
    "ë™ì•„ì¼ë³´": "https://rss.donga.com/total.xml",
    "í•œê²¨ë ˆ":  "https://www.hani.co.kr/rss/",
    "ê²½í–¥ì‹ ë¬¸": "https://www.khan.co.kr/rss/rssdata/total_news.xml",
    # ê²½ì œì§€
    "ë§¤ì¼ê²½ì œ": "https://www.mk.co.kr/rss/30000001/",
    "í•œêµ­ê²½ì œ": "https://www.hankyung.com/feed/all-news",
    "ë¨¸ë‹ˆíˆ¬ë°ì´": "https://rss.mt.co.kr/rss/mt_news.xml",
    # ìŠ¤íƒ€íŠ¸ì—…
    "ì¼€ì´ìŠ¤íƒ€íŠ¸ì—…": "https://www.k-startup.go.kr/web/contents/rss/startupnews.do",
}

# GitHub Actions í™˜ê²½ì—ì„œë„ í•­ìƒ ì ‘ê·¼ ê°€ëŠ¥í•œ ë³´ì¥ ì†ŒìŠ¤
GUARANTEED_SOURCES = ["êµ¬ê¸€ë‰´ìŠ¤", "êµ¬ê¸€ë‰´ìŠ¤_ê²½ì œ", "êµ¬ê¸€ë‰´ìŠ¤_IT"]

DEFAULT_PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì•„ì´ë””ì–´ ê¸°íšìì…ë‹ˆë‹¤.
ì•„ë˜ ì˜¤ëŠ˜ì˜ ì£¼ìš” ë‰´ìŠ¤ë“¤ì„ ê¹Šì´ ë¶„ì„í•˜ê³ , ì´ ë‰´ìŠ¤ë“¤ì˜ íë¦„ì„ ì°½ì˜ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ í˜ì‹ ì ì¸ ìƒˆë¡œìš´ ì•„ì´ë””ì–´ë¥¼ í•˜ë‚˜ ë„ì¶œí•´ì£¼ì„¸ìš”.

{news_block}

ë‹¤ìŒ 5ê°œ í•­ëª©ì„ ìˆœì„œëŒ€ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê° í•­ëª©ì€ ì•„ë˜ ë¼ë²¨ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.

ì•„ì´ë””ì–´ ì´ë¦„:
(ê°„ê²°í•˜ê³  ì¸ìƒì ì¸ ì´ë¦„)

í•µì‹¬ í†µì°°:
(ë‰´ìŠ¤ë“¤ì´ ì–´ë–¤ ê³µí†µëœ íë¦„ì´ë‚˜ ê¸°íšŒë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€ ì„¤ëª…)

ì•„ì´ë””ì–´ ì„¤ëª…:
(êµ¬ì²´ì ì¸ ì„œë¹„ìŠ¤/ì œí’ˆ/ì •ì±…/ìº í˜ì¸ ì•„ì´ë””ì–´)

ì‹¤í˜„ ë°©ì•ˆ:
(ë‹¨ê³„ë³„ ì ‘ê·¼ë²• ë˜ëŠ” ì£¼ìš” ì‹¤í–‰ í¬ì¸íŠ¸ 3ê°€ì§€)

ê¸°ëŒ€ íš¨ê³¼:
(ì´ ì•„ì´ë””ì–´ê°€ ê°€ì ¸ì˜¬ ë³€í™”ì™€ ê°€ì¹˜)

[ì£¼ì˜ì‚¬í•­]
- í‘œ(í…Œì´ë¸”) í˜•ì‹ ê¸ˆì§€, ê¸€ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ì„±
- ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ì™€ ì´ë¯¸ì§€ëŠ” ì‹œìŠ¤í…œì—ì„œ ìë™ ì²¨ë¶€ë˜ë¯€ë¡œ ì§ì ‘ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
- #, ##, *, ** ê°™ì€ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ìœ„ì˜ 5ê°œ í•­ëª©ë§Œ ì‘ì„±í•˜ì„¸ìš”."""

KST = timezone(timedelta(hours=9))


def _supabase_headers() -> dict:
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY", "")
    return {
        "apikey": key,
        "Authorization": f"Bearer {key}",
        "Content-Type": "application/json",
    }


# ---------------------------------------------------------------------------
# 0. ì„¤ì • ë¡œë“œ
# ---------------------------------------------------------------------------

def fetch_settings() -> dict:
    """Supabase agent_settings í…Œì´ë¸”ì—ì„œ ì„¤ì • ë¡œë“œ. ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜."""
    default = {
        "enabled": True,
        "run_every_hours": 1,
        "active_sources": list(ALL_SOURCES.keys()),
        "prompt_template": "",
        "news_interest_keywords": "",
        "news_exclude_keywords": "",
    }

    url = os.environ.get("SUPABASE_URL", "").rstrip("/")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY", "")

    if not url or not key:
        return default

    req = urllib.request.Request(
        f"{url}/rest/v1/agent_settings?id=eq.1",
        headers=_supabase_headers(),
    )
    try:
        with urllib.request.urlopen(req) as resp:
            data = json.loads(resp.read().decode())
            if data:
                return {**default, **data[0]}
    except Exception as e:
        print(f"ì„¤ì • ë¡œë“œ ì‹¤íŒ¨ (ê¸°ë³¸ê°’ ì‚¬ìš©): {e}", file=sys.stderr)

    return default


# ---------------------------------------------------------------------------
# 1. ë‰´ìŠ¤ ìˆ˜ì§‘
# ---------------------------------------------------------------------------

_BROWSER_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/124.0.0.0 Safari/537.36"
)


def _extract_image_url(entry) -> str:
    """RSS í•­ëª©ì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ URL ì¶”ì¶œ (media_thumbnail â†’ media_content â†’ enclosures ìˆœ)."""
    for thumb in entry.get("media_thumbnail", []):
        if thumb.get("url"):
            return thumb["url"]
    for media in entry.get("media_content", []):
        url = media.get("url", "")
        if url and "image" in media.get("type", "image"):
            return url
    for enc in entry.get("enclosures", []):
        if "image" in enc.get("type", "") and (enc.get("href") or enc.get("url")):
            return enc.get("href") or enc.get("url", "")
    return ""


def _resolve_google_news_url(url: str) -> str:
    """Google News URL â†’ ì‹¤ì œ ê¸°ì‚¬ URL (HTTP ë¦¬ë””ë ‰ì…˜ ì¶”ì , ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜)."""
    if "news.google.com" not in url:
        return url
    try:
        req = urllib.request.Request(url, headers={"User-Agent": _BROWSER_UA})
        with urllib.request.urlopen(req, timeout=10) as resp:
            final = resp.url
            return final if "news.google.com" not in final else url
    except Exception as e:
        print(f"    Google URL ë³€í™˜ ì‹¤íŒ¨: {e}", file=sys.stderr)
        return url


def _download_image(image_url: str) -> tuple:
    """ì´ë¯¸ì§€ URLì—ì„œ ë°”ì´ë„ˆë¦¬ ë°ì´í„° ë‹¤ìš´ë¡œë“œ. (bytes, content_type) ë°˜í™˜. ì‹¤íŒ¨ ì‹œ (b'', '')."""
    if not image_url:
        return b"", ""
    try:
        req = urllib.request.Request(image_url, headers={"User-Agent": _BROWSER_UA})
        with urllib.request.urlopen(req, timeout=15) as resp:
            ct = resp.headers.get("Content-Type", "image/jpeg").split(";")[0].strip()
            if not ct.startswith("image/"):
                return b"", ""
            return resp.read(), ct
    except Exception as e:
        print(f"    ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({image_url[:70]}): {e}", file=sys.stderr)
        return b"", ""


def _ensure_supabase_bucket(bucket: str) -> bool:
    """Supabase Storage ë²„í‚·ì´ ì—†ìœ¼ë©´ public ë²„í‚·ìœ¼ë¡œ ìƒì„±."""
    supa_url = os.environ.get("SUPABASE_URL", "").rstrip("/")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY", "")
    if not supa_url or not key:
        return False
    headers = {"apikey": key, "Authorization": f"Bearer {key}"}
    try:
        req = urllib.request.Request(f"{supa_url}/storage/v1/bucket/{bucket}", headers=headers)
        urllib.request.urlopen(req, timeout=10)
        return True  # ì´ë¯¸ ì¡´ì¬
    except urllib.error.HTTPError as e:
        if e.code not in (400, 404):
            return False
    except Exception:
        return False
    # ë²„í‚· ìƒì„±
    payload = json.dumps({"id": bucket, "name": bucket, "public": True}).encode()
    req2 = urllib.request.Request(
        f"{supa_url}/storage/v1/bucket",
        data=payload,
        headers={**headers, "Content-Type": "application/json"},
        method="POST",
    )
    try:
        urllib.request.urlopen(req2, timeout=10)
        print(f"  Supabase ë²„í‚· '{bucket}' ìƒì„± ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"  Supabase ë²„í‚· ìƒì„± ì‹¤íŒ¨: {e}", file=sys.stderr)
        return False


def _upload_image_to_supabase(image_data: bytes, filename: str, content_type: str) -> str:
    """ì´ë¯¸ì§€ë¥¼ Supabase Storageì— ì—…ë¡œë“œí•˜ê³  ê³µê°œ URL ë°˜í™˜. ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´."""
    supa_url = os.environ.get("SUPABASE_URL", "").rstrip("/")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY", "")
    if not supa_url or not key:
        return ""
    bucket = "news-images"
    req = urllib.request.Request(
        f"{supa_url}/storage/v1/object/{bucket}/{filename}",
        data=image_data,
        headers={
            "apikey": key,
            "Authorization": f"Bearer {key}",
            "Content-Type": content_type,
            "x-upsert": "true",
        },
        method="POST",
    )
    try:
        urllib.request.urlopen(req, timeout=30)
        public_url = f"{supa_url}/storage/v1/object/public/{bucket}/{filename}"
        print(f"    ì´ë¯¸ì§€ ì—…ë¡œë“œ ì™„ë£Œ: {filename}")
        return public_url
    except Exception as e:
        print(f"    ì´ë¯¸ì§€ ì—…ë¡œë“œ ì‹¤íŒ¨ ({filename}): {e}", file=sys.stderr)
        return ""


def process_news_images(news_items: list) -> None:
    """ê° ë‰´ìŠ¤ í•­ëª©ì˜ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ Supabase Storageì— ì—…ë¡œë“œ.
    news_items[i]['image_url']ì„ Supabase ê³µê°œ URLë¡œ êµì²´ (ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •)."""
    has_images = any(item.get("image_url") for item in news_items)
    if not has_images:
        print("  ì´ë¯¸ì§€ ì—†ìŒ â€” ê±´ë„ˆëœ€")
        return

    _ensure_supabase_bucket("news-images")
    date_prefix = datetime.now(KST).strftime("%Y%m%d")
    ext_map = {"image/jpeg": "jpg", "image/png": "png", "image/webp": "webp", "image/gif": "gif"}

    for i, item in enumerate(news_items):
        raw_url = item.get("image_url", "")
        if not raw_url:
            continue
        print(f"  [{i+1}] ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘: {raw_url[:70]}")
        image_data, ct = _download_image(raw_url)
        if not image_data:
            item["image_url"] = ""
            continue
        ext = ext_map.get(ct, "jpg")
        title_hash = hashlib.md5(item["title"].encode()).hexdigest()[:8]
        filename = f"{date_prefix}/{i+1}_{title_hash}.{ext}"
        item["image_url"] = _upload_image_to_supabase(image_data, filename, ct)


def fetch_top_news(active_sources: list, count: int = 3) -> list:
    """í™œì„±í™”ëœ RSS í”¼ë“œì—ì„œ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘. ì†ŒìŠ¤ë³„ë¡œ ìˆœí™˜í•˜ë©° countê°œ ìˆ˜ì§‘."""
    news_items = []
    cutoff = datetime.now(timezone.utc) - timedelta(days=3)

    # active_sources ê¸°ë°˜ í”¼ë“œ ëª©ë¡ (ALL_SOURCES ìˆœì„œ ìœ ì§€)
    feeds = [(name, url) for name, url in ALL_SOURCES.items() if name in active_sources]

    # Supabase ì„¤ì •ì— ì—†ë”ë¼ë„ êµ¬ê¸€ë‰´ìŠ¤ëŠ” í•­ìƒ ì•ì— ì¶”ê°€ (GitHub Actionsì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥)
    guaranteed_feeds = [
        (name, ALL_SOURCES[name]) for name in GUARANTEED_SOURCES
        if name not in active_sources and name in ALL_SOURCES
    ]
    feeds = guaranteed_feeds + feeds

    # ì¼ë¶€ í•œêµ­ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ëŠ” ë´‡ User-Agent ì°¨ë‹¨ â†’ ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ìœ„ì¥
    ua_headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        )
    }

    for source_name, feed_url in feeds:
        if len(news_items) >= count:
            break
        try:
            print(f"  [{source_name}] ìˆ˜ì§‘ ì¤‘...")

            # íƒ€ì„ì•„ì›ƒ 15ì´ˆ ì„¤ì • (ë¬´í•œ ëŒ€ê¸° ë°©ì§€)
            prev_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(15)
            try:
                feed = feedparser.parse(feed_url, request_headers=ua_headers)
            finally:
                socket.setdefaulttimeout(prev_timeout)

            # ë„¤íŠ¸ì›Œí¬/íŒŒì‹± ì˜¤ë¥˜ë¡œ í•­ëª©ì´ ì—†ëŠ” ê²½ìš°
            if not feed.entries:
                reason = str(getattr(feed, "bozo_exception", "í•­ëª© ì—†ìŒ"))
                print(f"  [{source_name}] ê±´ë„ˆëœ€ â€” {reason}", file=sys.stderr)
                continue

            # í•œ ì†ŒìŠ¤ì—ì„œ í•„ìš”í•œ ë§Œí¼ ì—¬ëŸ¬ ê¸°ì‚¬ ìˆ˜ì§‘ (ê¸°ì¡´: 1ê°œ ê³ ì • â†’ ê°œì„ : ìµœëŒ€ neededê°œ)
            needed = count - len(news_items)
            collected = 0
            for entry in feed.entries:
                if collected >= needed:
                    break
                parsed_time = entry.get("published_parsed") or entry.get("updated_parsed")
                pub_time_str = ""
                if parsed_time:
                    pub_dt = datetime(*parsed_time[:6], tzinfo=timezone.utc)
                    if pub_dt < cutoff:
                        continue  # ë„ˆë¬´ ì˜¤ë˜ëœ ê¸°ì‚¬ ê±´ë„ˆëœ€
                    pub_time_str = pub_dt.astimezone(KST).strftime("%m/%d %H:%M")
                title = entry.get("title", "").strip()
                summary = entry.get("summary", entry.get("description", title)).strip()
                link = entry.get("link", "")
                # Google News ë‹¨ì¶• URL â†’ ì‹¤ì œ ê¸°ì‚¬ URL ë³€í™˜
                if "news.google.com" in link:
                    link = _resolve_google_news_url(link)
                summary = re.sub(r"<[^>]+>", "", summary).strip()
                if len(summary) > 500:
                    summary = summary[:497] + "..."
                if not title:
                    continue
                news_items.append({
                    "source": source_name,
                    "title": title,
                    "summary": summary,
                    "link": link,
                    "published_at": pub_time_str,
                    "image_url": _extract_image_url(entry),
                })
                collected += 1

            if collected > 0:
                print(f"  [{source_name}] OK: {collected}ê°œ ìˆ˜ì§‘")
            else:
                print(f"  [{source_name}] ìµœê·¼ ë‰´ìŠ¤ ì—†ìŒ", file=sys.stderr)

        except Exception as e:
            print(f"  [{source_name}] ì‹¤íŒ¨: {e}", file=sys.stderr)

    if len(news_items) < count:
        print(f"ê²½ê³ : {count}ê°œ ì¤‘ {len(news_items)}ê°œë§Œ ìˆ˜ì§‘ë¨", file=sys.stderr)

    return news_items[:count]


# ---------------------------------------------------------------------------
# 2. í‚¤ì›Œë“œ ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ / ì œì™¸ í•„í„°
# ---------------------------------------------------------------------------

def fetch_news_by_interest_keywords(interest_keywords: str, count: int = 12) -> list:
    """ê´€ì‹¬ í‚¤ì›Œë“œë³„ë¡œ Google News ê²€ìƒ‰ RSSë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ë‰´ìŠ¤ ìˆ˜ì§‘.

    í‚¤ì›Œë“œê°€ ì—¬ëŸ¬ ê°œë©´ ê°ê° ê²€ìƒ‰ í›„ í•©ì³ì„œ ìµœì‹ ìˆœ ì •ë ¬, ì¤‘ë³µ ì œê±°.
    """
    keywords = [k.strip() for k in interest_keywords.split(",") if k.strip()]
    if not keywords:
        return []

    ua_headers = {"User-Agent": _BROWSER_UA}
    cutoff = datetime.now(timezone.utc) - timedelta(days=3)
    per_kw = max(4, (count + len(keywords) - 1) // len(keywords))  # í‚¤ì›Œë“œë‹¹ ìˆ˜ì§‘ ëª©í‘œ

    all_items: list = []
    seen_titles: set = set()

    for keyword in keywords:
        encoded = urllib.parse.quote(keyword)
        search_url = f"https://news.google.com/rss/search?q={encoded}&hl=ko&gl=KR&ceid=KR:ko"
        print(f"  [ê²€ìƒ‰] '{keyword}' ...")
        try:
            prev_timeout = socket.getdefaulttimeout()
            socket.setdefaulttimeout(15)
            try:
                feed = feedparser.parse(search_url, request_headers=ua_headers)
            finally:
                socket.setdefaulttimeout(prev_timeout)

            collected = 0
            for entry in feed.entries:
                if collected >= per_kw:
                    break
                title = entry.get("title", "").strip()
                if not title or title in seen_titles:
                    continue
                parsed_time = entry.get("published_parsed") or entry.get("updated_parsed")
                pub_dt = None
                pub_time_str = ""
                if parsed_time:
                    pub_dt = datetime(*parsed_time[:6], tzinfo=timezone.utc)
                    if pub_dt < cutoff:
                        continue
                    pub_time_str = pub_dt.astimezone(KST).strftime("%m/%d %H:%M")
                summary = entry.get("summary", entry.get("description", title)).strip()
                link = entry.get("link", "")
                if "news.google.com" in link:
                    link = _resolve_google_news_url(link)
                summary = re.sub(r"<[^>]+>", "", summary).strip()
                if len(summary) > 500:
                    summary = summary[:497] + "..."
                seen_titles.add(title)
                all_items.append({
                    "source": f"êµ¬ê¸€ë‰´ìŠ¤({keyword})",
                    "title": title,
                    "summary": summary,
                    "link": link,
                    "published_at": pub_time_str,
                    "image_url": _extract_image_url(entry),
                    "_pub_dt": pub_dt or datetime.min.replace(tzinfo=timezone.utc),
                })
                collected += 1
            print(f"    â†’ {collected}ê°œ ìˆ˜ì§‘")
        except Exception as e:
            print(f"    ê²€ìƒ‰ ì‹¤íŒ¨: {e}", file=sys.stderr)

    # ìµœì‹ ìˆœ ì •ë ¬ í›„ ì„ì‹œ í•„ë“œ ì œê±°
    all_items.sort(key=lambda x: x["_pub_dt"], reverse=True)
    for item in all_items:
        item.pop("_pub_dt", None)
    return all_items[:count]


def apply_exclude_filter(news_candidates: list, exclude_keywords: str) -> list:
    """ì œì™¸ í‚¤ì›Œë“œê°€ ì œëª©/ìš”ì•½ì— í¬í•¨ëœ ë‰´ìŠ¤ë¥¼ ì œê±°."""
    exclude_kws = [k.strip().lower() for k in exclude_keywords.split(",") if k.strip()] if exclude_keywords else []
    if not exclude_kws:
        return news_candidates

    result = []
    excluded_count = 0
    for item in news_candidates:
        text = (item["title"] + " " + item["summary"]).lower()
        if any(kw in text for kw in exclude_kws):
            excluded_count += 1
        else:
            result.append(item)
    if excluded_count:
        print(f"  ì œì™¸ í‚¤ì›Œë“œë¡œ {excluded_count}ê°œ ë‰´ìŠ¤ ì œê±°")
    return result


# ---------------------------------------------------------------------------
# 3. í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ì„ ë³„ (í‚¤ì›Œë“œ í•„í„°ë§ ì´í›„ ì ìš©)
# ---------------------------------------------------------------------------

def filter_news_by_prompt(news_candidates: list, prompt_template: str, count: int = 3) -> list:
    """prompt_templateì˜ ì£¼ì œ/ì˜ë„ì— ë§ëŠ” ë‰´ìŠ¤ë¥¼ Claude AIë¡œ ì„ ë³„.
    prompt_templateê°€ ì—†ìœ¼ë©´ í›„ë³´ ì•ì—ì„œ countê°œë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜."""
    if not prompt_template or not prompt_template.strip():
        return news_candidates[:count]

    client = anthropic.Anthropic()

    candidates_text = "\n".join(
        f"{i+1}. [{item['source']}] {item['title']}\n   {item['summary'][:200]}"
        for i, item in enumerate(news_candidates)
    )

    selection_prompt = f"""ë‹¤ìŒì€ ìˆ˜ì§‘ëœ ë‰´ìŠ¤ í›„ë³´ ëª©ë¡ì…ë‹ˆë‹¤:

{candidates_text}

---
ì•„ë˜ ì‚¬ìš©ì ì§€ì¹¨ì„ ì½ê³ , ì§€ì¹¨ì— ë§ê²Œ ë‰´ìŠ¤ë¥¼ ì„ ë³„í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§€ì¹¨:
{prompt_template[:800]}

[ì„ ë³„ ê·œì¹™]
1. ì§€ì¹¨ì— "ê¸ˆì§€", "ì œì™¸", "ë¹¼ì¤˜", "í•˜ì§€ ë§ˆ" ê°™ì€ í‘œí˜„ì´ ìˆìœ¼ë©´ í•´ë‹¹ ì£¼ì œì˜ ë‰´ìŠ¤ëŠ” ì ˆëŒ€ ì„ íƒí•˜ì§€ ë§ˆì„¸ìš”.
2. ê¸ˆì§€ ì¡°ê±´ì„ ë¨¼ì € ì ìš©í•´ í›„ë³´ë¥¼ ê±°ë¥¸ ë’¤, ë‚˜ë¨¸ì§€ ì¤‘ì—ì„œ ì§€ì¹¨ì˜ ê´€ì‹¬ì‚¬ì— ê°€ì¥ ì˜ ë§ëŠ” ë‰´ìŠ¤ë¥¼ ì„ íƒí•˜ì„¸ìš”.
3. ì •í™•íˆ {count}ê°œë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ê¸ˆì§€ ì¡°ê±´ ì ìš© í›„ ì í•©í•œ ë‰´ìŠ¤ê°€ {count}ê°œ ë¯¸ë§Œì´ë©´, ê¸ˆì§€ ì¡°ê±´ì„ ìœ„ë°˜í•˜ì§€ ì•ŠëŠ” ë²”ìœ„ì—ì„œ ë‚¨ì€ ë‰´ìŠ¤ ì¤‘ ìµœì„ ì˜ ê²ƒì„ ì±„ì›Œ ì£¼ì„¸ìš”.

ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš” (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ì—†ì´):
{{"selected": [1, 3, 5]}}

selectedëŠ” 1ë¶€í„° ì‹œì‘í•˜ëŠ” ë²ˆí˜¸ ë°°ì—´ì´ë©° ì •í™•íˆ {count}ê°œì—¬ì•¼ í•©ë‹ˆë‹¤."""

    try:
        message = client.messages.create(
            model="claude-haiku-4-5-20251001",
            max_tokens=100,
            messages=[{"role": "user", "content": selection_prompt}],
        )
        text = message.content[0].text.strip()
        m = re.search(r'\{[\s\S]*\}', text)
        if m:
            result = json.loads(m.group())
            indices = [int(x) - 1 for x in result.get("selected", [])]
            selected = [news_candidates[i] for i in indices if 0 <= i < len(news_candidates)]
            if len(selected) == count:
                print(f"  ì„ ë³„ëœ ë‰´ìŠ¤: {[news_candidates[i]['title'][:30] for i in indices]}")
                return selected
            print(f"  ì„ ë³„ ê²°ê³¼ ìˆ˜ ë¶ˆì¼ì¹˜ ({len(selected)}ê°œ) â€” ì•ì—ì„œ {count}ê°œ ì‚¬ìš©", file=sys.stderr)
    except Exception as e:
        print(f"  ë‰´ìŠ¤ ì„ ë³„ ì˜¤ë¥˜: {e} â€” ì•ì—ì„œ {count}ê°œ ì‚¬ìš©", file=sys.stderr)

    return news_candidates[:count]


# ---------------------------------------------------------------------------
# 4. AI ì•„ì´ë””ì–´ ìƒì„±
# ---------------------------------------------------------------------------

def generate_idea(news_items: list, prompt_template: str = "") -> str:
    """Claude AIë¡œ ë‰´ìŠ¤ 3ê°œë¥¼ ê²°í•©í•˜ì—¬ ì•„ì´ë””ì–´ ìƒì„±."""
    client = anthropic.Anthropic()

    news_block = "\n\n".join(
        f"**ë‰´ìŠ¤ {i + 1} ({item['source']}"
        + (f", {item['published_at']}" if item.get('published_at') else "")
        + f")**\nì œëª©: {item['title']}\në‚´ìš©: {item['summary']}"
        for i, item in enumerate(news_items)
    )

    template = prompt_template.strip() if prompt_template and prompt_template.strip() else DEFAULT_PROMPT_TEMPLATE

    if "{news_block}" in template:
        prompt = template.replace("{news_block}", news_block)
    else:
        prompt = template + "\n\n" + news_block

    message = client.messages.create(
        model="claude-opus-4-5",
        max_tokens=2048,
        messages=[{"role": "user", "content": prompt}],
    )

    return message.content[0].text


# ---------------------------------------------------------------------------
# 5. Supabase ì €ì¥
# ---------------------------------------------------------------------------

def save_to_supabase(news_items: list, idea: str, generated_at: datetime) -> bool:
    """Supabase news_ideas í…Œì´ë¸”ì— ê²°ê³¼ ì €ì¥ (REST API ì‚¬ìš©)."""
    url = os.environ.get("SUPABASE_URL", "").rstrip("/")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY", "")

    if not url or not key:
        print("ê²½ê³ : SUPABASE_URL ë˜ëŠ” SUPABASE_SERVICE_ROLE_KEYê°€ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
        return False

    payload = json.dumps({
        "generated_at": generated_at.isoformat(),
        "news_items": news_items,
        "idea": idea,
    }).encode("utf-8")

    req = urllib.request.Request(
        f"{url}/rest/v1/news_ideas",
        data=payload,
        headers={**_supabase_headers(), "Prefer": "return=minimal"},
        method="POST",
    )

    try:
        with urllib.request.urlopen(req) as resp:
            print(f"Supabase ì €ì¥ ì™„ë£Œ (status: {resp.status})")
            return True
    except urllib.error.HTTPError as e:
        body = e.read().decode()
        print(f"Supabase ì €ì¥ ì‹¤íŒ¨ ({e.code}): {body}", file=sys.stderr)
        return False


# ---------------------------------------------------------------------------
# 6. í…”ë ˆê·¸ë¨ ë°œì†¡
# ---------------------------------------------------------------------------

def _send_telegram_photos(token: str, chat_id: str, news_items: list) -> None:
    """ë‰´ìŠ¤ í•­ëª©ë³„ ì´ë¯¸ì§€ë¥¼ í…”ë ˆê·¸ë¨ sendPhotoë¡œ ê°œë³„ ë°œì†¡.
    ìº¡ì…˜ì— ì œëª© + ì›ë¬¸ ë§í¬ í¬í•¨. Supabase Storage URLì„ ì´ë¯¸ì§€ë¡œ ì‚¬ìš©."""
    for i, item in enumerate(news_items):
        img = item.get("image_url", "")
        if not img:
            continue
        pub = f" {item['published_at']}" if item.get('published_at') else ""
        caption = f"[{item['source']}]{pub} {item['title']}"
        payload = json.dumps({
            "chat_id": chat_id,
            "photo": img,
            "caption": caption,
        }).encode("utf-8")
        req = urllib.request.Request(
            f"https://api.telegram.org/bot{token}/sendPhoto",
            data=payload,
            headers={"Content-Type": "application/json"},
            method="POST",
        )
        try:
            with urllib.request.urlopen(req, timeout=20) as resp:
                result = json.loads(resp.read().decode())
                if result.get("ok"):
                    print(f"  [{i+1}] ì´ë¯¸ì§€ ë°œì†¡ ì™„ë£Œ")
                else:
                    print(f"  [{i+1}] ì´ë¯¸ì§€ ë°œì†¡ ì‹¤íŒ¨: {result.get('description')}", file=sys.stderr)
        except Exception as e:
            print(f"  [{i+1}] ì´ë¯¸ì§€ ë°œì†¡ ì˜¤ë¥˜: {e}", file=sys.stderr)


def send_to_telegram(news_items: list, idea: str, generated_at: datetime) -> bool:
    """í…”ë ˆê·¸ë¨ ë´‡ìœ¼ë¡œ ê²°ê³¼ ë©”ì‹œì§€ ë°œì†¡."""
    token = os.environ.get("TELEGRAM_BOT_TOKEN", "")
    chat_id = os.environ.get("TELEGRAM_CHAT_ID", "")

    if not token or not chat_id:
        print("ê²½ê³ : TELEGRAM_BOT_TOKEN ë˜ëŠ” TELEGRAM_CHAT_IDê°€ ì—†ìŠµë‹ˆë‹¤.", file=sys.stderr)
        return False

    # ì‹¤ì œ ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ ë¨¼ì € ì´ë¯¸ì§€ ê·¸ë£¹ìœ¼ë¡œ ì „ì†¡
    _send_telegram_photos(token, chat_id, news_items)

    timestamp = generated_at.strftime("%Yë…„ %mì›” %dì¼ %H:%M (KST)")

    # ì•„ì´ë””ì–´ í›„ì²˜ë¦¬: ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±° / ë¼ë²¨ ë³¼ë“œ ë³€í™˜
    idea = re.sub(r'\(https?://[^\)]+\)', '', idea)
    idea = re.sub(r'ğŸ–¼ï¸[^\n]*\n?', '', idea)
    idea = re.sub(r'^#{1,6}\s+', '', idea, flags=re.MULTILINE)   # ## ì œëª© â†’ ì œëª©
    idea = re.sub(r'\*\*(.+?)\*\*', r'\1', idea)                 # **êµµê²Œ** â†’ êµµê²Œ
    idea = re.sub(r'\*(.+?)\*', r'\1', idea)                     # *ê¸°ìš¸ì„* â†’ ê¸°ìš¸ì„
    # ì˜ˆìƒì¹˜ ëª»í•œ ì„¹ì…˜ í—¤ë” ì œê±° (5ê°œ ë¼ë²¨ ì™¸ì˜ ì§§ì€ `:` ì¤„)
    idea = re.sub(
        r'^(?!(ì•„ì´ë””ì–´ ì´ë¦„|í•µì‹¬ í†µì°°|ì•„ì´ë””ì–´ ì„¤ëª…|ì‹¤í˜„ ë°©ì•ˆ|ê¸°ëŒ€ íš¨ê³¼):).{1,40}:\s*$',
        '',
        idea,
        flags=re.MULTILINE,
    )
    idea = re.sub(
        r'^(ì•„ì´ë””ì–´ ì´ë¦„|í•µì‹¬ í†µì°°|ì•„ì´ë””ì–´ ì„¤ëª…|ì‹¤í˜„ ë°©ì•ˆ|ê¸°ëŒ€ íš¨ê³¼):',
        r'<b>\1</b>',
        idea,
        flags=re.MULTILINE,
    )
    idea = re.sub(r'\n{3,}', '\n\n', idea).strip()

    # ë‰´ìŠ¤ ì„¹ì…˜: ì œëª© + ìš”ì•½ + ì›ë¬¸ ë§í¬
    news_parts = []
    for i, item in enumerate(news_items):
        pub = f"  {item['published_at']}" if item.get('published_at') else ""
        title = item['title']
        summary = item.get('summary', '').strip()
        link = item.get('link', '')
        src = item['source']

        part = f"{i + 1}. <b>{title}</b>\n{src}{pub}"
        if summary:
            short = summary if len(summary) <= 180 else summary[:177] + "..."
            part += f"\n{short}"
        if link:
            part += f"\n<a href=\"{link}\">ì›ë¬¸ ë³´ê¸°</a>"
        news_parts.append(part)

    news_block = "\n\n".join(news_parts)

    message = (
        f"<b>ë‰´ìŠ¤ ì•„ì´ë””ì–´ ë¦¬í¬íŠ¸</b>  {timestamp}\n\n"
        f"<b>ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤ ë¸Œë¦¬í•‘</b>\n\n"
        f"{news_block}\n\n"
        f"â€”â€”\n\n"
        f"{idea[:2000]}"
    )

    payload = json.dumps({
        "chat_id": chat_id,
        "text": message,
        "parse_mode": "HTML",
        "disable_web_page_preview": True,
    }).encode("utf-8")

    req = urllib.request.Request(
        f"https://api.telegram.org/bot{token}/sendMessage",
        data=payload,
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    try:
        with urllib.request.urlopen(req) as resp:
            result = json.loads(resp.read().decode())
            if result.get("ok"):
                print("í…”ë ˆê·¸ë¨ ë°œì†¡ ì™„ë£Œ")
                return True
            else:
                print(f"í…”ë ˆê·¸ë¨ ë°œì†¡ ì‹¤íŒ¨: {result}", file=sys.stderr)
                return False
    except Exception as e:
        print(f"í…”ë ˆê·¸ë¨ ë°œì†¡ ì˜¤ë¥˜: {e}", file=sys.stderr)
        return False


# ---------------------------------------------------------------------------
# ë©”ì¸
# ---------------------------------------------------------------------------

def main():
    print("=== ë‰´ìŠ¤ ì•„ì´ë””ì–´ ì—ì´ì „íŠ¸ ì‹œì‘ ===\n")

    print("[ì„¤ì •] Supabaseì—ì„œ ì„¤ì • ë¡œë“œ ì¤‘...")
    settings = fetch_settings()

    if not settings["enabled"]:
        print("ì—ì´ì „íŠ¸ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        sys.exit(0)

    run_every = int(settings["run_every_hours"])
    is_manual = os.environ.get("GITHUB_EVENT_NAME") == "workflow_dispatch"
    if run_every > 1 and not is_manual:
        current_hour_utc = datetime.now(timezone.utc).hour
        if current_hour_utc % run_every != 0:
            print(f"í˜„ì¬ {current_hour_utc}ì‹œ (UTC) â€” {run_every}ì‹œê°„ ê°„ê²© ë¯¸í•´ë‹¹. ê±´ë„ˆëœ€.")
            sys.exit(0)
    if is_manual:
        print("ìˆ˜ë™ ì‹¤í–‰ â€” ì‹œê°„ ê°„ê²© ì²´í¬ ê±´ë„ˆëœ€.")

    active_sources = settings["active_sources"]
    prompt_template = settings.get("prompt_template", "")
    interest_keywords = settings.get("news_interest_keywords", "")
    exclude_keywords = settings.get("news_exclude_keywords", "")
    has_custom_prompt = bool(prompt_template and prompt_template.strip())
    has_keywords = bool(interest_keywords or exclude_keywords)
    print(f"í™œì„± ì†ŒìŠ¤: {active_sources} | ì‹¤í–‰ ì£¼ê¸°: {run_every}ì‹œê°„")
    print(f"ì»¤ìŠ¤í…€ í”„ë¡¬í”„íŠ¸: {'ìˆìŒ' if has_custom_prompt else 'ì—†ìŒ (ê¸°ë³¸ê°’ ì‚¬ìš©)'}")
    if interest_keywords:
        print(f"ê´€ì‹¬ í‚¤ì›Œë“œ: {interest_keywords}")
    if exclude_keywords:
        print(f"ì œì™¸ í‚¤ì›Œë“œ: {exclude_keywords}")
    print()

    # [1/6] ë‰´ìŠ¤ ìˆ˜ì§‘: ê´€ì‹¬ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ Google News ê²€ìƒ‰ìœ¼ë¡œ ì§ì ‘ ìˆ˜ì§‘, ì—†ìœ¼ë©´ ì¼ë°˜ RSS
    candidate_count = 12 if (has_custom_prompt or exclude_keywords) else 3
    if interest_keywords:
        print(f"[1/6] ê´€ì‹¬ í‚¤ì›Œë“œ ê²€ìƒ‰ìœ¼ë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        news_candidates = fetch_news_by_interest_keywords(interest_keywords, count=12)
        if not news_candidates:
            print("  ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ â€” ì¼ë°˜ RSSë¡œ ëŒ€ì²´ ìˆ˜ì§‘í•©ë‹ˆë‹¤.", file=sys.stderr)
            news_candidates = fetch_top_news(active_sources, candidate_count)
    else:
        print(f"[1/6] ë‰´ìŠ¤ í›„ë³´ {candidate_count}ê°œ ìˆ˜ì§‘ ì¤‘...")
        news_candidates = fetch_top_news(active_sources, candidate_count)

    if not news_candidates:
        print("ì˜¤ë¥˜: ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", file=sys.stderr)
        sys.exit(1)
    print(f"ìˆ˜ì§‘ ì™„ë£Œ: {len(news_candidates)}ê°œ\n")

    print("[2/6] ì œì™¸ í‚¤ì›Œë“œ í•„í„°ë§ ì¤‘...")
    news_candidates = apply_exclude_filter(news_candidates, exclude_keywords)
    if not news_candidates:
        print("ê²½ê³ : ì œì™¸ í•„í„°ë§ í›„ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. í•„í„° ì—†ì´ ì¬ìˆ˜ì§‘í•©ë‹ˆë‹¤.", file=sys.stderr)
        news_candidates = fetch_top_news(active_sources, 3)
    print(f"í•„í„°ë§ ì™„ë£Œ: {len(news_candidates)}ê°œ\n")

    print("[3/6] í”„ë¡¬í”„íŠ¸ ê¸°ë°˜ ë‰´ìŠ¤ ì„ ë³„ ì¤‘...")
    news_items = filter_news_by_prompt(news_candidates, prompt_template, count=3)
    print(f"ì„ ë³„ ì™„ë£Œ: {len(news_items)}ê°œ\n")

    print("[4/6] ë‰´ìŠ¤ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ (ë‹¤ìš´ë¡œë“œ â†’ Supabase Storage)...")
    process_news_images(news_items)
    print()

    print("[5/6] Claude AIë¡œ ì•„ì´ë””ì–´ ìƒì„± ì¤‘...")
    idea = generate_idea(news_items, prompt_template)
    print("ìƒì„± ì™„ë£Œ\n")

    generated_at = datetime.now(KST)

    print("[6/6] ê²°ê³¼ ì €ì¥ & ë°œì†¡ ì¤‘...")
    supabase_ok = save_to_supabase(news_items, idea, generated_at)
    telegram_ok = send_to_telegram(news_items, idea, generated_at)

    print("\n=== ì™„ë£Œ ===")
    print(f"  Supabase: {'OK' if supabase_ok else 'FAIL'}")
    print(f"  Telegram: {'OK' if telegram_ok else 'FAIL'}")

    if not supabase_ok and not telegram_ok:
        sys.exit(1)


if __name__ == "__main__":
    main()
